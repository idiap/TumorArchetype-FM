#
# SPDX-FileCopyrightText: Copyright Â© 2025 Idiap Research Institute <contact@idiap.ch>
#
# SPDX-FileContributor: Lisa Fournier <lisa.fournier@idiap.ch>
#
# SPDX-License-Identifier: GPL-3.0-only
#

import torch
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import CosineAnnealingLR
from itertools import cycle
from copy import deepcopy
import numpy as np
from sklearn.metrics import confusion_matrix
import os
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import KFold
import seaborn as sns
from matplotlib.colors import LogNorm

import torch.nn as nn
import torch.optim as optim

import matplotlib.pyplot as plt

class HeadModule(nn.Module):
    def __init__(self, in_features, out_features):
        super(HeadModule, self).__init__()
        self.fc1 = nn.Linear(in_features, in_features // 2)
        self.fc2 = nn.Linear(in_features // 2, out_features)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x


def train_linear_classifier(
    train_dataset,
    test_dataset,
    val_dataset=None,
    input_dim=1,
    output_dim=1,
    num_iterations=12500,
    device="cpu",
    classes=None,
):
    """
    Trains a linear classifier on the frozen embeddings generated by a model.

    Args:
    - train_dataset (TensorDataset): Training dataset with embeddings and labels.
    - val_dataset (TensorDataset): Validation dataset with embeddings and labels.
    - test_dataset (TensorDataset): Test dataset with embeddings and labels.
    - input_dim (int): Dimensionality of the frozen embeddings (input to the linear classifier).
    - num_classes (int): Number of classes for classification.
    - num_iterations (int): Number of iterations to train the classifier.
    - device (str): Device to run the model on (e.g., 'cuda' or 'cpu').

    Returns:
    - accuracy (float): Accuracy on the validation set after training.
    - model (nn.Module): Trained linear classifier model.
    - confusion_matrix (ndarray): Confusion matrix of the best model on the test set.
    """

    # Create DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
    if val_dataset:
        val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)
    infinite_train_loader = cycle(train_loader)

    # Initialize the model, loss function, optimizer, and scheduler
    model = HeadModule(input_dim, output_dim).to(
        device
    )  # Output dimension is 1 for binary classification
    if output_dim == 1:
        criterion = nn.BCEWithLogitsLoss()
    else:
        criterion = nn.CrossEntropyLoss()

    optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=0.0)
    scheduler = CosineAnnealingLR(optimizer, T_max=num_iterations)

    # Training loop
    best_val_loss = float("inf")
    torch.cuda.empty_cache()
    best_accuracy = 0.0
    best_test_loss = float("inf")
    best_conf_matrix = None
    test = 0
    for iteration in range(num_iterations):
        model.train()
        embeddings, labels = next(infinite_train_loader)
        embeddings, labels = embeddings.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(embeddings)

        if output_dim == 1:
            loss = criterion(
                outputs.squeeze(), labels.float()
            )  # BCE requires float labels
        else:
            loss = criterion(
                outputs, labels.long()
            )  # CrossEntropy requires integer class labels

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        # Update the learning rate
        scheduler.step()

        # Validate the model every 500 iterations
        if iteration == test:
            test = int(50 + test * 1.05)
            total = 0
            correct = 0
            test_loss = 0.0

            all_labels = []
            all_predictions = []
            with torch.no_grad():
                for embeddings, labels in test_loader:
                    embeddings, labels = (
                        embeddings.to(device),
                        labels.to(device).squeeze(),
                    )
                    outputs = model(embeddings)

                    if output_dim == 1:
                        loss = criterion(
                            outputs.squeeze(), labels.float()
                        )  # BCE requires float labels
                    else:
                        loss = criterion(
                            outputs, labels.long()
                        )  # CrossEntropy requires integer class labels

                    test_loss += loss.item()

                    if output_dim == 1:
                        predicted = torch.sigmoid(outputs).round().squeeze()
                    else:
                        predicted = torch.argmax(outputs, dim=1)

                    total += labels.size(0)
                    correct += predicted.eq(labels).sum().item()

                    # Accumulate labels and predictions
                    all_labels.extend(labels.cpu().numpy())
                    all_predictions.extend(predicted.cpu().numpy())

            # Calculate average test loss and accuracy
            test_loss /= len(test_loader)
            accuracy = 100.0 * correct / total

            # Convert to NumPy arrays
            all_labels = np.array(all_labels)
            all_predictions = np.array(all_predictions)

            # Calculate confusion matrix
            # Save the best model checkpoint
            if best_accuracy < accuracy:
                best_test_loss = test_loss
                print("Saving the best model checkpoint")
                best_model_state = deepcopy(model.state_dict())
                best_accuracy = accuracy
                best_conf_matrix = confusion_matrix(
                    all_labels, all_predictions, labels=range(output_dim)
                )
            # Update best accuracy and confusion matrix
            print(
                f"Test Loss: {test_loss} Test Accuracy: {accuracy} Best Loss: {best_test_loss} Best Accuracy: {best_accuracy}%"
            )

    print("Training completed.")

    # Load the best model checkpoint
    model.load_state_dict(best_model_state)
    if val_dataset:
        model.eval()
        val_loss = 0.0
        correct_val = 0
        total_val = 0
        all_val_labels = []
        all_val_predictions = []
        with torch.no_grad():
            for embeddings, labels in val_loader:
                embeddings = embeddings.to(device)
                labels = labels.to(device)
                outputs = model(embeddings)
                loss = criterion(outputs.squeeze(), labels.float())
                val_loss += loss.item()
                predicted = torch.sigmoid(outputs).round().squeeze()
                total_val += labels.size(0)
                correct_val += predicted.eq(labels.float()).sum().item()
                all_val_labels.extend(labels.cpu().numpy())
                all_val_predictions.extend(predicted.cpu().numpy())

            val_loss /= len(val_loader)
            val_accuracy = 100.0 * correct_val / total_val

            # Save the best confusion matrix for validation
            if val_accuracy > best_accuracy:
                best_accuracy = val_accuracy
                best_conf_matrix = confusion_matrix(
                    all_val_labels, all_val_predictions, labels=classes
                )

    return best_conf_matrix, best_accuracy, model


class Classification_mlp():

    """
        EmbClassification is a utility class that provides functionality for performing 
        classification tasks using embeddings saved into AnnData format. It includes methods for multi-layer perceptron (MLP) 
        classification with 5-fold cross-validation and visualization of classification results.
        Attributes:
            emb (AnnData): AnnData object containing embeddings data and labels for classification in the "obs".
        Methods:
            __init__(emb=None):
                Initializes the EmbClassification with optional embeddings.
            mlp_classification(save_file, label_column="label"):
                Performs MLP classification on the embeddings using 5-fold cross-validation. 
                Saves the trained models, confusion matrix, and bar plots of prediction distributions.
                    save_file (str): Path to save the classification results, including models 
                                    and visualizations.
                    label_column (str): Column name in the AnnData object containing the labels for classification.
                    Default is "label".
        Example:
            emb_classification = EmbClassification(emb=ad_adata)
            emb_classification.mlp_classification(save_file="results", label_column="cell_type")
    """


    def __init__(self,
                 emb=None):
        
        self.emb = emb

    def mlp_classification(self, save_file, label_column="label"):
        """MLP classification of the embeddings using 5-fold cross-validation. The embeddings are split into train and test set. The MLP is trained on the train set and tested on the test set.

        Args:
            save_file (str): Path to save the classification results.
        """
        emb = self.emb[~self.emb.obs[label_column].isna()]
        encoder = LabelEncoder()
        encoder.fit(emb.obs[label_column])

        X = torch.tensor(emb.X, dtype=torch.float)
        y = torch.tensor(encoder.transform(emb.obs[label_column]), dtype=torch.long)

        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        fold = 1
        all_confusion_matrices = []
        all_accuracies = []

        for train_index, test_index in kf.split(X):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            train_dataset = TensorDataset(X_train, y_train)
            test_dataset = TensorDataset(X_test, y_test)

            input_dim = X_train.shape[1]
            output_dim = len(encoder.classes_)
            classes = [str(cls) for cls in encoder.classes_]

            confusion_matrix, accuracy, model = train_linear_classifier(
                train_dataset,
                test_dataset,
                input_dim=input_dim,
                output_dim=output_dim,
                num_iterations=12500,
                classes=classes,
            )

            all_confusion_matrices.append(confusion_matrix)
            all_accuracies.append(accuracy)

            torch.save(model, os.path.join(save_file, f"mlp_fold_{fold}.pth"))
            fold += 1

        # Average confusion matrix and accuracy
        avg_confusion_matrix = np.mean(all_confusion_matrices, axis=0)
        avg_accuracy = np.mean(all_accuracies)

        # Save the confusion matrix as an image in a PDF
        fig, ax = plt.subplots(figsize=(output_dim + 5, output_dim + 5))
        sns.heatmap(
            avg_confusion_matrix,
            annot=True,
            fmt=".1f",
            cmap="Blues",
            ax=ax,
            norm=LogNorm(),
            xticklabels=classes,
            yticklabels=classes,
        )
        ax.set_xlabel("Predicted Labels")
        ax.set_ylabel("True Labels")
        ax.set_title(
            "Confusion Matrix (Avg) - Global Accuracy: {:.4f}".format(avg_accuracy)
        )
        plt.savefig(os.path.join(save_file, "confusion_matrix_avg.pdf"))
        plt.close()

        num_classes = len(classes)
        fig, axes = plt.subplots(num_classes, 1, figsize=(8, 4 * num_classes))

        if num_classes == 1:
            axes = [axes]  # Ensure axes is iterable if there is only one class

        for i, (true_label, ax) in enumerate(zip(classes, axes)):
            # Get total number of predictions for this true label
            total_predictions = int(avg_confusion_matrix[i].sum())

            # Normalize row to get percentages
            row_sum = avg_confusion_matrix[i].sum()
            percentages = (avg_confusion_matrix[i] / row_sum) * 100 if row_sum > 0 else np.zeros_like(avg_confusion_matrix[i])

            # Sort indices based on percentage values (descending order)
            sorted_indices = np.argsort(percentages)[::-1]
            sorted_classes = [classes[j] for j in sorted_indices]
            sorted_percentages = percentages[sorted_indices]

            # Plot bar chart
            sns.barplot(x=sorted_classes, y=sorted_percentages, ax=ax, palette="Blues_r")

            ax.set_ylim(0, 100)
            ax.set_ylabel("Percentage (%)")
            ax.set_xlabel("Predicted Labels")
            ax.set_title(f"Prediction Distribution for True Label {true_label} with {total_predictions} predictions)")

            # Add text labels on bars
            for p, value in zip(ax.patches, sorted_percentages):
                ax.annotate(f"{value:.1f}%", (p.get_x() + p.get_width() / 2, p.get_height()), ha="center", va="bottom")

            ax.set_xticklabels(sorted_classes, rotation=45)

        plt.tight_layout()
        plt.savefig(os.path.join(save_file, "confusion_matrix_barplots.pdf"))
        plt.close()